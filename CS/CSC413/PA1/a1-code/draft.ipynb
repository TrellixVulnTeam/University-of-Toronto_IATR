{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('data.pk','rb'))\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 250\n",
    "\n",
    "def calculate_log_co_occurence(word_data):\n",
    "  \"Compute the log-co-occurence matrix for our data.\"\n",
    "  log_co_occurence = np.zeros((vocab_size, vocab_size))\n",
    "  for input in word_data:\n",
    "    log_co_occurence[input[0], input[1]] += 1\n",
    "    log_co_occurence[input[1], input[2]] += 1\n",
    "    # If we want symmetric co-occurence can also increment for these.\n",
    "    # Optional: How would you generalize the model if our target co-occurence isn't symmetric?\n",
    "    log_co_occurence[input[1], input[0]] += 1\n",
    "    log_co_occurence[input[2], input[1]] += 1\n",
    "  delta_smoothing = 0.5  # A hyperparameter.  You can play with this if you want.\n",
    "  log_co_occurence += delta_smoothing  # Add delta so log doesn't break on 0's.\n",
    "  log_co_occurence = np.log(log_co_occurence)\n",
    "  return log_co_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_co_occurence_train = calculate_log_co_occurence(data['train_inputs'])\n",
    "log_co_occurence_valid = calculate_log_co_occurence(data['valid_inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.1 * np.random.normal(size=(250, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.ones((1,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.803360380906535"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_co_occurence_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_GLoVE(W, b, log_co_occurence):\n",
    "  \"Compute the GLoVE loss.\"\n",
    "  n,_ = log_co_occurence.shape\n",
    "  return np.sum((W @ W.T + b @ np.ones([1,n]) + np.ones([n,1])@b.T - log_co_occurence)**2)\n",
    "\n",
    "def grad_GLoVE(W,  b, log_co_occurence):\n",
    "  \"Return the gradient of GLoVE objective w.r.t W and b.\"\n",
    "  \"INPUT: W - Vxd; b - Vx1; log_co_occurence: VxV\"\n",
    "  \"OUTPUT: grad_W - Vxd; grad_b - Vx1\"\n",
    "    n,_ = log_co_occurence.shape\n",
    "  ###########################   YOUR CODE HERE  ##############################\n",
    "    grad_W = []\n",
    "    for i in range(len(W)):\n",
    "        res = np.zeros(W.shape[1])\n",
    "        for j range(len(W)):\n",
    "            res += (W[j].T @ W[i] + b[j] + b[i] - log_co_occurence[i][j])@ W[j]\n",
    "        grad_W.append([4*res])\n",
    "    grad_W = np.array(d_W)\n",
    "  ############################################################################\n",
    "  return grad_W, grad_b\n",
    "\n",
    "def train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=False):\n",
    "  \"Traing W and b according to GLoVE objective.\"\n",
    "  n,_ = log_co_occurence_train.shape\n",
    "  learning_rate = 0.5 / n  # A hyperparameter.  You can play with this if you want.\n",
    "  for epoch in range(n_epochs):\n",
    "    grad_W, grad_b = grad_GLoVE(W, b, log_co_occurence_train)\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "    train_loss, valid_loss = loss_GLoVE(W, b, log_co_occurence_train), loss_GLoVE(W, b, log_co_occurence_valid)\n",
    "    if do_print:\n",
    "      print(f\"Train Loss: {train_loss}, valid loss: {valid_loss}, grad_norm: {np.sum(grad_w**2)}\")\n",
    "  return W, b, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n_epochs = 500  # A hyperparameter.  You can play with this if you want.\n",
    "embedding_dims = np.array([1,3,5,7,10,12,15,20,25,30,40,50])  # Play with this\n",
    "final_train_losses, final_val_losses = [], []  # Store the final losses for graphing\n",
    "W_final_2d, b_final_2d = None, None\n",
    "do_print = False  # If you want to see diagnostic information during training\n",
    "for embedding_dim in tqdm(embedding_dims):\n",
    "  init_variance = 0.1  # A hyperparameter.  You can play with this if you want.\n",
    "  W = init_variance * np.random.normal(size=(250, embedding_dim))\n",
    "  b = init_variance * np.random.normal(size=(250, 1))\n",
    "  if do_print:\n",
    "    print(f\"Training for embedding dimension: {embedding_dim}\")\n",
    "  W_final, b_final, train_loss, valid_loss = train_GLoVE(W, b, log_co_occurence_train, log_co_occurence_valid, n_epochs, do_print=do_print)\n",
    "  if embedding_dim == 2:\n",
    "    # Save a parameter copy if we are training 2d embedding for visualization later\n",
    "    W_final_2d = W_final\n",
    "    b_final_2d = b_final\n",
    "  final_train_losses += [train_loss]\n",
    "  final_val_losses += [valid_loss]\n",
    "  if do_print:\n",
    "    print(f\"Final validation loss: {valid_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.loglog(embedding_dims, final_train_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Training Loss\")\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.loglog(embedding_dims, final_val_losses)\n",
    "pylab.xlabel(\"Embedding Dimension\")\n",
    "pylab.ylabel(\"Validation Loss\")\n",
    "pylab.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
