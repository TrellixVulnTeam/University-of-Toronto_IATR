============================================================================
CSC 373                 Lecture Summary for Week 10                Fall 2014
============================================================================

READINGS: Section 34.4, 34.5.
SELF-TEST: Trace the reductions covered during last week's lectures on two
    examples for each reduction: one where the answer is True and the other
    where the answer is False.

Template for proofs of NP-completeness: To show A is NPc, prove that

    A in NP: Describe a polytime verifier for A.
            "Given (x,c), check c has correct format and properties..."
        Argue that verifier runs in polytime and that
        x is a yes-instance iff verifier outputs "yes" for some c.

    Note that all problems in NP we've seen so far have a similar structure
    to their definition: "the answer for object A is Yes iff there is some
    related object B such that some property holds about A and B" -- for
    example, for CLIQUE: "the answer for undirected graphs G and integers k
    is Yes iff there is a subset of vertices C that forms a k-clique in G".
    For all such problems, the verifier will also have a common structure:
    "on input (A,c), check that c encodes an object B and that A and B have
    the required property". Because of the way these decision problems are
    defined, this guarantees (A,c) is accepted for some c iff A is a
    yes-instance. All that remains is to ensure checking property of A,B can
    be done in polytime.

    A is NP-hard: Show B <=p A for some NP-hard B.
            "Given x, construct y_x as follows: ..."
        Argue that construction can be carried out in polytime and that
        x yes-instance iff y_x yes-instance (often by showing x yes-instance
        => y_x yes-instance and y_x yes-instance => x yes-instance)
        In more detail, this involves:
          . starting with arbitrary input y for B (i.e., without making any
            assumption about whether y is a yes-instance or a no-instance),
          . describing explicit construction of specific input x_y for A,
          . arguing construction can be carried out in polytime,
          . arguing if y is a yes-instance, then so is x_y,
          . arguing if x_y is a yes-instance, then so was y (or
            equivalently, if y is a no-instance, then so is x_y).
        Watch last step! Argument starts from x_y constructed earlier (not
        from arbitrary input x for A), and relates it to arbitrary y that
        x_y was constructed from.

    Traps to watch out for:
      . Direction of reduction: must start from arbitrary input x for B
        (cannot place any restrictions on input; reduction must work with
        all possible inputs) and explicitly construct specific input y_x for
        A.
      . "Reduction" that does something different for yes-instances vs.
        no-instances: this would involve telling the difference, which can't
        be done in polytime when B is NP-hard.

One more example of reduction, to show interesting ideas: SAT <=p 3SAT.

  - Input for reduction? Formula \phi.
    Output of reduction? Formula \phi' in 3-CNF.
    Relationship? \phi satisfiable iff \phi' satisfiable.

  - Trick: introduce new variable for each connective in \phi; write clauses
    that express "value of new variable = value of connective"; \phi' is
    conjunction of all these clauses together with clause z_1, where z_1 is
    new variable for main connective in \phi.

  - Example: (x_1 /\ x_2) => ~x_3.
    Use z_1 for =>, z_2 for /\, z_3 for ~.
    \phi' = z_1 /\ (z_1 <=> (z_2 => z_3))
                /\ (z_2 <=> (x_1 /\ x_2))
                /\ (z_3 <=> ~x_3)

  - Replace each "pseudoclause" (a <=> A) with up to eight clauses as
    follows: write down truth table for (a <=> A); for each line where
    (a <=> A) = False, write clause that is False for that line.
    For example: a <=> (b /\ c)
                 0  1   0    0   no clause (line is True)
                 0  1   0    1   no clause (line is True)
                 0  1   1    0   no clause (line is True)
                 0  0   1    1   clause:   a \/ ~b \/ ~c
                 1  0   0    0   clause:  ~a \/  b \/  c
                 1  0   0    1   clause:  ~a \/  b \/ ~c
                 1  0   1    0   clause:  ~a \/ ~b \/  c
                 1  1   1    1   no clause (line is True)

  - Final \phi' = conjunction of each clause, repeating literals as needed
    in clauses with fewer than three literals (e.g., pseudoclause "z_1"
    becomes (z_1 \/ z_1 \/ z_1)).

  - Runtime for reduction?
    O(m) -- scan all of \phi for each pseudoclause
    O(m) -- replace each pseudoclause by up to 8 clauses

  - If \phi satisfiable, then assign values to new variables to match values
    of each connective in \phi -- this satisfies \phi'.
    If \phi' satisfiable, then by construction each new variable has value
    equal to some connective in \phi; because of clause z_1, this means \phi
    is satisfied.

-----------------
Self-reducibility
-----------------

We've focused on decision problems, but many problems are more naturally
"search problems": given input x, find solution y.

Examples:
  - Given prop. formula F, find satisfying assignment, if one exists.
  - Given graph G, integer k, find a clique of size k in G, if one exists.
  - Given graph G, find a Ham. path in G, if one exists.
  - Given set of numbers S, target t, find subset of S whose sum equals t,
    if one exists.
  - etc.

Notation: We say A is "Turing-reducible" to B in polynomial time (written
A -p-> B) iff:
  . assuming algorithm S_B solves B in constant time,
  . we can write an algorithm S_A to solve A
    (by making appropriate calls to S_B), where
  . S_A runs in worst-case polynomial time.
This does _not_ require that B is solvable in polynomial-time.

Clearly, efficient solution to search problem would give efficient solution
to corresponding decision problem. So proof that decision problem is NP-hard
implies that search problem is "NP-hard" as well (in some generalized sense
of NP-hard), and does not have efficient solution.

Example: Clique-Decision -p-> Clique-Search:
  - Suppose CLS(G,k) returns a clique of size k in G (or NIL),
    in constant time.
  -     CLD(G,k):
            return CLS(G,k) != NIL
    solves Clique-Decision, also in constant time.

But exactly how much more difficult are search problems?

Perhaps surprisingly, many are only polynomially more difficult than
corresponding decision problem, in the following sense: any efficient
solution to the decision problem can be used to solve the search problem
efficiently. This is called "self-reducibility".

Example 1: CLIQUE-SEARCH
    Input: Undirected graph G, positive integer k.
    Output: A clique of size k in G, if one exists; special value NIL if
        there is no such clique in G.

  - Assumption: There is an algorithm CL(G,k) that returns True iff G
    contains a clique of size k -- i.e., CL solves the decision problem.

    WARNING! The argument for self-reducibility is NOT that "any algorithm
    that solves the decision problem must include a part that solves the
    search problem", as this is in fact not always true. For example, there
    is an algorithm that can determine whether or not an integer has any
    factors (i.e., whether or not it belongs to COMPOSITES) without actually
    finding any of the factors (through some fairly involved number theory
    about properties of prime numbers).

    In this case, for example, it would be a circular argument to assume
    that the algorithm CL must find a clique of size k in order to return
    whether or not such a clique exists. Instead, what we must do is show
    how to write a different algorithm that searches for a k-clique in G, by
    making use of the information provided by calls to CL.

  - Idea: For each vertex in turn, remove it iff resulting graph still
    contains a k-clique.

  - Details: We construct an algorithm to solve CLIQUE-SEARCH as follows.

    CLS(G,k):
        if not CL(G,k):  return NIL  # no k-clique in G
        for each vertex v in V:
            # remove v and its incident edges
            V' = V - {v};  E' = E - { (u,v) : u in V }
            # check if there is still a k-clique
            if CL(G'=(V',E'),k):
                # v not required for k-clique, leave it out
                V = V';  E = E'
        return V

  - Correctness: CL(G=(V,E),k) remains true at every step so at the end, V
    contains every vertex in a k-clique of G. At the same time, every other
    vertex will be taken out because it is not required, so V will contain
    no other vertex. Hence, the value returned is a k-clique of G.

  - Runtime: Each vertex of G examined once, and one call to CL for each
    one, plus linear amount of additional work (removing edges). Total is
    O((n+1)*t(n,m) + n*(n+m)) where t(n,m) is runtime of CL on graphs with
    n vertices and m edges; this is polytime if t(n,m) is polytime.

  - Exercise: What happens if G contains more than one k-clique?

General technique to prove self-reducibility:
  - assume hypothetical algorithm to solve decision problem,
  - write algorithm to solve search problem by making calls to decision
    problem algorithm (possibly many calls on many different inputs),
  - make sure that search problem algorithm runs in polytime if decision
    problem algorithm does -- argue at most polynomially many calls to
    subroutine are made and at most polytime spent outside those calls.

Example 2: HAMPATH-SEARCH
    Input: Graph G, vertices s,t.
    Output: A Ham. path in G from s to t.

  - Idea 1: For each vertex in turn, remove it iff resulting graph still
    contains a Ham. path.

    Problem: Every vertex must be in the path anyway, and this does not say
    where to put each vertex (which edges to use to travel through this
    vertex).

  - Idea 2: Remove s and its edges. Then consider each neighbour of s (must
    keep track of them separately), find one that has a Ham. path to t and
    remove it and its edges. Repeat until t is reached.
    Potential for exponential number of paths, but only polynomially many
    will be examined -- with HP decision algorithm, only consider paths
    guaranteed to be Hamiltonian.

  - Idea 3: For each edge in turn, remove it iff resulting graph still
    contains a Ham. path -- same as for CLIQUE above, except considering
    edges one-by-one instead of vertices.

    Both ideas work, although their runtime is slightly different and the
    last one is simpler.

Example 3: VERTEX-COVER-SEARCH
    Input: Graph G, integer k.
    Output: A vertex cover of size k, if one exists (NIL otherwise).

  - Idea 1: Remove vertices one-by-one as long as resulting graph still
    contains a vertex cover of size k.

  - Problem: If G contains a VC of size k, then G-v (remove v and all
    incident edges) also contains a VC of size k, whether or not v is in the
    cover (unless n < k, trivial to solve)!

  - Idea 2: Check if G-v contains a VC of size (k-1).

  - Algorithm:

        VCS(G,k):
            if not VC(G,k):  return NIL
            C = {}  # the vertices in a vertex cover of G
            for each vertex v in V, and while k > 0:
                if VC(G-v, k-1):
                    C = C U {v};  G = G - v;  k = k - 1
            return C

  - Correctness: Loop invariant: G contains a VC of size k. At each
    iteration,
      . if G-v contains a VC of size (k-1), then G contains a VC of size k
        that includes v: say C' is VC of size (k-1) in G-v, then C' U {v} is
        a VC of size k in G;
      . if G contains a VC C of size k that includes v, then G-v contains a
        VC C - {v} of size (k-1); taking the contrapositive: if G-v does not
        contain a VC of size (k-1), then v does not belong to any VC of size
        k in G.

  - Runtime: O((n+1)*t(n,m) + n*(n+m)) -- for each vertex v, we perform one
    call to VC in time t(n,m) and compute G-v in time O(n+m).

