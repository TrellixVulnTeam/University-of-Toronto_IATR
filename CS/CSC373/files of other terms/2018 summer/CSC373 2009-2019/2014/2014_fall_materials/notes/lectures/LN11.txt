============================================================================
CSC 373                 Lecture Summary for Week 11                Fall 2014
============================================================================

READINGS: Sections 35.intro, 35.1, 35.2.
SELF-TEST: Exercise 35.1-1.

Self-reducibility (cont'd) -- optimization problems:

  - Some search problems with one or more numerical parameters naturally
    occur in practice in the form of optimization problems, e.g.,

      . MAX-CLIQUE
      . MIN-VERTEX-COVER
      . etc.

    Optimization problems can also be polytime self-reducible by using
    decision problem algorithm to find optimal value of relevant
    parameter(s).

    Example: MAX-CLIQUE
        Idea: Given G, perform binary search in range [1,n] by making calls
        to CL(G,k) for various values of k, in order to find maximum value k
        such that G contains a k-clique but no (k+1)-clique. Then, use
        search algorithm to find k-clique. This makes O(log n) calls to CL
        in addition to time to find the clique.
        (Note: Linear search for value of k would also be OK because search
        is in the range [1..n] and input size = n.)

    Similar idea would work for MIN-VERTEX-COVER, etc.

---------------------------
Coping with NP-completeness
---------------------------

So far...

  - Techniques for writing (hopefully) efficient algorithms
    (greedy, dynamic programming, network flows, linear programming).

  - Techniques for showing that problems cannot be solved efficiently
    (if P != NP, then every NP-complete problem D does not belong to P).

  - Traditional point of view:
      . P = "easy"
      . NP-complete = "hard"

  - But:
      . definition of NP-completeness based on worst-case analysis
        (maybe inputs encountered in practice are rarely worst-case)
      . for "real-world" input sizes (>= 10^6), even n^3 runtime is
        inefficient

------------------------
Approximation algorithms
------------------------

For NP-hard problems D, not possible to solve D *exactly* and *in polytime*
(unless P = NP).

In practice, sometimes sacrifice on efficiency: use exponential-time
algorithm and hope inputs don't trigger worst-case behaviour. Particularly
useful on restricted families of inputs:

  - A problem that's hard in general may be easy for inputs of a certain
    type. For example:
      . 2SAT.
      . Independent set on a tree.
      . Many graph problems are easy on trees or other restricted kinds of
        graphs. Others not (e.g., planar 3-colouring still NP-complete).

Sometimes sacrifice on exactness, particularly when D is optimization
problem: instead of searching for *best* solution, settle for "good enough"
solution. But what does that mean exactly?

For minimization problems, let OPT be the minimum value of any solution.
Suppose we have an approximation algorithm that generates solutions with
approximate value A. By definition, OPT <= A for all inputs (since OPT is
minimum).

The "approximation ratio" of our algorithm is a function r(n) such that
    A <= r(n) * OPT    for all n,
i.e., approximation ratio gives a bound on how much larger than optimum our
approximate value might be -- it gives a guarantee that approximate values
cannot be "too" large compared to optimum.

Vertex Cover:

  - Approx algo 1:
    Repeatedly pick an edge and put both endpoints in C, then remove all
    edges incident on the two endpoints, until no edge remains. Then,
        |C| <= 2 * OPT
    because all covers include at least one endpoint from every edge in C
    (all edges in C are disjoint, i.e., with no endpoint in common) so in
    particular, OPT >= |C|/2. This shows approximation ratio <= 2.
    To show approximation ratio = 2, need an example where algorithm
    performs that badly -- in this case, use n disjoint edges! Algorithm
    returns 2n endpoints but n of them are sufficient.

  - Approx algo 2:
      . Create linear program from input graph
      . Compute optimal solution to linear program: x*_1, x*_2, ..., x*_n.
      . Create cover as follows:
            for each v_i in V, put v_i in C iff x*_i >= 1/2.
        (C is a cover because constraint x_i + x_j >= 1 guarantees at least
        one of x*_i, x*_j >= 1/2 for each edge (v_i,v_j).)
    Approximation ratio?
    Consider minimum vertex cover C'. For i = 1,...,n, let x'_i = 1 if v_i
    in C'; x'_i = 0 otherwise. x'_1, ..., x'_n is a solution to linear
    program that satisfies all constraints with 0-1 values so
                   |C'| = \sum x'_i >= \sum x*_i
    where x*_i is optimal solution to linear program with no restriction on
    values, so guaranteed to be at least as small as any other solution,
    including those with additional restrictions.
    For i = 1,...,n, let x~_i = 1 if x*_i >= 1/2; x~_i = 0 othewise.
    Then, for each i, x~_i <= 2 x*_i so
        |C| = \sum x~_i <= 2 \sum x*_i <= 2 |C'| (by equation above)
    Hence, |C| is no more than twice the size of a minimum vertex cover.

 Q: In general, how can we compute ratio without knowing OPT?
    (Particularly for NP-hard problems, like VC, TSP, bin packing?)
 A: Use a lower bound. Find another value LB that's easy to compute and for
    which you can prove LB <= OPT and A <= r * LB, as in approx algo 2 for
    vertex cover.

How well can problems be approximated?
Even though all NP-complete problems "equivalent" to each other (in one
sense), approximation ratios all over the place.
  - VC: approx ratio 2.
  - Knapsack: approx ratio 1+e in time O(n^3/e), for all constants
    e in (0,1]!
  - TSP: no constant ratio, unless P=NP!

Traveling Salesman Problem (TSP):

  - Given *complete* graph G (i.e., that contains every possible edge,
    directed or not) with edge weights w(e), find a tour (simple cycle
    including all vertices) with minimum total weight.

  - NP-hard: no polytime solution.

  - NP-hard to approximate with constant ratio:
    Suppose we have an algorithm with approx ratio C, i.e., guaranteed to
    find tour with total weight <= C * OPT. We show how this algorithm could
    be used to solve an NP-hard problem.
    Given an input G to HamCycle problem, construct an instance of TSP G' as
    follows: on the same vertex set as G, put in all possible edges with
    w(e) = 1 if e in G, w(e) = Cn+1 if e not in G.
    Any tour in G' that uses only edges with weight 1 has total weight n;
    any tour in G' that uses at least one edge with weight Cn+1 has total
    weight > Cn.
    If G contains a Ham cycle, then G' contains a tour with total weight n;
    if G does not contain a Ham cycle, then all tours in G' have total
    weight > Cn+1 (they must contain at least one edge not in G).
    So now, run approx. algorithm on G'. If algo returns answer with total
    weight <= C*n, then G contains a Ham cycle; if algo returns answer with
    total weight > Cn+1, then G does not contain a Ham cycle. As this solves
    the Ham cycle problem, approx algorithm cannot run in polytime.

  - Special case: TSP with triangle inequality
    (w(u,v) <= w(u,w) + w(w,v)  for all u,v,w in V)
    has a 2-approximation algorithm!

     1. Construct MST of G, T.
     2. Construct Eulerian tour of G travelling along each edge of T
        once in each direction, starting from arbitrary leaf f in T.
     3. Construct tour of G from Eulerian tour:
          - start with current node c = f and mark it as "visited"
            (all other nodes "unvisited")
          - repeat:
                let n be next node in Eulerian tour
                if n is unvisited:
                    add edge (c,n) to cycle
                    let c = n and mark it as visited
                # else do nothing (continue with next node n)
            until n = f
          - add edge (c,f) -- this closes the tour because
            c is the last node visited

    [Example -- sorry, too difficult to draw in ASCII. Output is equivalent
    to version in textbook: order vertices according to preorder traversal
    of T.]

  - Approximation ratio:
      . Consider any optimum tour C* and edge e in C* with max weight. C*-e
        is a Ham. path in G, which is a special kind of spanning tree. Since
        T is a MST, cost(C*) >= cost(C*-e) >= cost(T).
      . Consider the tour C found by the algorithm. Then, cost(C) <=
        2 * cost(T) Since C is obtained from an Eulerian cycle based on T
        (with cost exactly 2 * cost(T)) by replacing paths (u,w_1),
        (w_1,w_2), ..., (w_k,v) with the edge (u,v), something that can only
        make cost(C) smaller, by the triangle inequality.
      . Putting these two facts together,
            cost(C) <= 2 * cost(T) <= 2 * cost(C*),
        i.e., the algorithm has approx. ratio at most 2.

  - A similar idea starting from a perfect matching instead of a MST yields
    an algorithm with approx ratio 3/2, but the algorithm and proof of
    approximation ratio are both more complicated.

